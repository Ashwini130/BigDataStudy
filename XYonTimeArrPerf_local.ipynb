{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = spark.sparkContext.textFile('hdfs://localhost:9000/XYonTimeArrPerf/part-r-00000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABE ATL DL\\t4.804878048780488',\n",
       " 'ABE CLT US\\t-0.30337078651685395',\n",
       " 'ABE CVG DL\\t-6.345238095238095',\n",
       " 'ABQ AMA WN\\t7.504201680672269',\n",
       " 'ABQ ATL DL\\t1.729281767955801',\n",
       " 'ABQ COS DL\\t3.164705882352941',\n",
       " 'ABQ CVG DL\\t6.670454545454546',\n",
       " 'ABQ DAL WN\\t7.606260296540363',\n",
       " 'ABQ DEN UA\\t5.16722972972973',\n",
       " 'ABQ DFW AA\\t4.222648752399232']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ABE', 'ATL', 'DL', '4.804878048780488'],\n",
       " ['ABE', 'CLT', 'US', '-0.30337078651685395'],\n",
       " ['ABE', 'CVG', 'DL', '-6.345238095238095'],\n",
       " ['ABQ', 'AMA', 'WN', '7.504201680672269'],\n",
       " ['ABQ', 'ATL', 'DL', '1.729281767955801'],\n",
       " ['ABQ', 'COS', 'DL', '3.164705882352941'],\n",
       " ['ABQ', 'CVG', 'DL', '6.670454545454546'],\n",
       " ['ABQ', 'DAL', 'WN', '7.606260296540363'],\n",
       " ['ABQ', 'DEN', 'UA', '5.16722972972973'],\n",
       " ['ABQ', 'DFW', 'AA', '4.222648752399232']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.map(lambda line:line.split()).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ABQ', 'DFW', 'AA', '4.222648752399232'],\n",
       " ['ABQ', 'DFW', 'DL', '5.601449275362318']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.map(lambda line:line.split()).filter(lambda tuple:tuple[0]=='ABQ').filter(lambda tuple:tuple[1]=='DFW').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.map(lambda line:line.split()).filter(lambda tuple:tuple[0]=='ABQ').filter(lambda tuple:tuple[1]=='DFW') #.map(lambda tuple:(float(tuple[3]),tuple[2])).sortByKey(ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = file.map(lambda tuple:(tuple[0],tuple[1],float(tuple[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ABQ', 'DFW', 4.222648752399232), ('ABQ', 'DFW', 5.601449275362318)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['127.0.0.1'])  # provide contact points and port\n",
    "session = cluster.connect('aviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"INSERT INTO airport_srcdest_arrival (src, dest, arr_delay) VALUES (?, ?, ?)\"\n",
    "#prepared = session.prepare(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.execute(prepared, (row.select('src'), row.select('dest'),row.select('arr_delay')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd3 = file1.map(lambda row: Row(src=row[0], dest=row[1], arr_delay=row[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+---+\n",
      "|        arr_delay|dest|src|\n",
      "+-----------------+----+---+\n",
      "|4.222648752399232| DFW|ABQ|\n",
      "|5.601449275362318| DFW|ABQ|\n",
      "+-----------------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'format_number' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-2a71e3f782f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arr_delay\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marr_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'format_number' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round, col\n",
    "df = df.withColumn(\"arr_delay\", format_number(df.arr_delay, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-1a6ce2362cd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cassandra-driver in c:\\users\\ashwini\\anaconda3\\lib\\site-packages (3.24.0)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in c:\\users\\ashwini\\anaconda3\\lib\\site-packages (from cassandra-driver) (0.2.1.post1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\ashwini\\anaconda3\\lib\\site-packages (from cassandra-driver) (1.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\ashwini\\anaconda3\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cassandra-driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(['127.0.0.1'])  # provide contact points and port\n",
    "session = cluster.connect('aviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = session.execute('select * from airport_srcdest_arrival;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(src='APT', dest='SDF', arr_delay=Decimal('-9.8'))\n"
     ]
    }
   ],
   "source": [
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"INSERT INTO airport_srcdest_arrival (src, dest, arr_delay) VALUES (?, ?, ?)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared = session.prepare(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x56800b5ac8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(prepared, (\"DAD\", \"SDA\", -2.34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(src='APT', dest='SDF', arr_delay=Decimal('-9.8'))\n",
      "Row(src='DAD', dest='SDA', arr_delay=Decimal('-2.339999999999999857891452847979962825775146484375'))\n"
     ]
    }
   ],
   "source": [
    "rows = session.execute('select * from airport_srcdest_arrival;')\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sqlContext.read \\\n",
    "  .format('org.apache.spark.sql.cassandra') \\\n",
    "  .options(table='airport_srcdest_arrival', keyspace='aviation') \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "|src|dest|           arr_delay|\n",
      "+---+----+--------------------+\n",
      "|ABQ| DFW|4.222648752399232000|\n",
      "|ABQ| DFW|5.601449275362318000|\n",
      "+---+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-83284e7fd42a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"org.apache.spark.sql.cassandra\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'append'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"airport_srcdest_arrival\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"aviation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.write\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".mode('append')\\\n",
    ".options(table = \"airport_srcdest_arrival\", keyspace = \"aviation\")  \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
